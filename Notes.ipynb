{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3\n",
    "\n",
    "** Agent **\n",
    "** Environment **\n",
    "** Policy **\n",
    "A stochastic rule by which the agent selectss actions as a function of states.\n",
    "\n",
    "**Belman Equation**\n",
    "Not sure what this is, but required to prove that a DP solution is optimal?\n",
    "Seems to have a unique solution for MDP, and helps in figuring out how to solve it.\n",
    "\n",
    "**Optimal Policy**\n",
    "There is always one policy that is better than or equal to all other policies.  \n",
    "An important part of RL is findin the optimal policy (or something close to it).\n",
    "One way to find this is to calculate the V* the optimal value at each state, then just use a greedy algorithm as our policy.\n",
    "\n",
    "**Markov Decision Process**\n",
    "An environment where the Markov property holds (i.e. p(next state) is determined entirely by present state.\n",
    "\n",
    "**Backup Operations**\n",
    "Not so sure about these?\n",
    "\n",
    "**Value Functions**\n",
    "\n",
    "We need to estimate the value at a particular state (state, action) pair.  This is the ev of all rewards in the future from that state (assuming we follow our current policy).\n",
    "\n",
    "We're using V(s) for expected return at state\n",
    "and Q(s,a) for expected return for (state, value) pair.\n",
    "\n",
    "\n",
    "** How to beat RL algorithms **\n",
    "Most RL algorithms apply heuristics, and so don't have accurate value information in states they are unlikely to encounter.  Hence, one strategy is to move the game into one of these unlikely states. (pg81)\n",
    "\n",
    "## Chapter 4\n",
    "\n",
    "**Policy Improvement Theorem**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
