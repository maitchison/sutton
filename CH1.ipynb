{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sutton \n",
    " \n",
    "## Chapter 1 - The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter Summary\n",
    "\n",
    "Just an introduction into machine learning using tic-tac-toe as an example.  One very interesting mention was John Andraea, who was a professor at UC (the university I study at), and who invented one of the first trial-and-error algorithms.\n",
    "\n",
    "This big takeaway is reinforcement learning is all about learning via trial-and-error in an environment. (\"learn by doing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exersizes\n",
    "\n",
    "**Ex1 Self-Play**\n",
    "Self-play may fail to explore certain areas of the problem space.  For example, if the algorithm never learns a certain strategy, it will never get any experience against that strategy.  One way to beat this kind of algorithm is to perfect an unusual strategy that has very poor returns at low-level skill.  As this kind of play will have been filtered out as low return.\n",
    "\n",
    "**Ex2 Symmetries**\n",
    "One aspect we must model is the opponent's perception of the game.  If the opponent does not have a symmetrical model (i.e. prefers the top left corner) then symmetric positions are not equivalent when playing this game.  Therefore it is best to not take advantage of the symmetry unless we can guarantee our opponent does the same.\n",
    "\n",
    "**Ex3 Greedy Play**\n",
    "Greedy play is not good for learning as it will not explore parts of the environment are only accessible via a non-greedy move.  Also, the task is to optimize for long term reward, which may involve making 'downward' steps to open up opportunities later on. (for example giving up a piece in Chess to gain a position advantage).\n",
    "\n",
    "**Ex4 Learning from Exploration**\n",
    "Not sure about this one?  If we include exploration then the probabilities will converge to the true probability of winning at each state.  If we do not then they will converge to something else, but I'm not sure what.  The exploration method will learn better, but the no-exploration may win more games. \n",
    "\n",
    "** Ex5ï¼š Other Improvements**\n",
    "It's hard to improve on the tic-tac-toe problem as it's quite easily solvable.  I.e. simply memorise which moves guarantee a non-loss, at every state.\n",
    "If we are playing against humans it would be worth trying to learn the types of mistakes people make and exploit those.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
